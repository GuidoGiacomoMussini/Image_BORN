{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xC6NSkP5uGJ8"
      ],
      "authorship_tag": "ABX9TyPBOi+VgPxEPsmwRUnB4i8c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuidoGiacomoMussini/Image_BORN/blob/main/Dictionary_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB4uX2t5ASKo",
        "outputId": "fdac1359-102d-4870-d2ce-0da50e40da25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm as progress_bar\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import scipy.sparse\n",
        "import numpy\n",
        "from sklearn.utils import shuffle\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.decomposition import DictionaryLearning\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#born"
      ],
      "metadata": {
        "id": "xC6NSkP5uGJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nEiBAzkG-kmq"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse\n",
        "import numpy\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import _check_sample_weight\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.exceptions import NotFittedError\n",
        "\n",
        "class BornClassifier(ClassifierMixin, BaseEstimator):\n",
        "    \"\"\"Scikit-learn implementation of Born's Classifier\n",
        "\n",
        "    This class is compatible with the [scikit-learn](https://scikit-learn.org) ecosystem.\n",
        "    It supports both dense and sparse input and GPU-accelerated computing via [CuPy](https://cupy.dev).\n",
        "    This classifier is suitable for classification with non-negative feature vectors.\n",
        "    The data `X` are treated as unnormalized probability distributions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    a : float\n",
        "        Amplitude. Must be strictly positive.\n",
        "    b : float\n",
        "        Balance. Must be non-negative.\n",
        "    h : float\n",
        "        Entropy. Must be non-negative.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    gpu_ : bool\n",
        "        Whether the model was fitted on GPU.\n",
        "    corpus_ : array-like of shape (n_features_in_, n_classes)\n",
        "        Fitted corpus.\n",
        "    classes_ : ndarray of shape (n_classes,)\n",
        "        Unique classes labels.\n",
        "    n_features_in_ : int\n",
        "        Number of features seen during `fit`.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, a=0.5, b=1., h=1.):\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.h = h\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"Fit the classifier according to the training data X, y.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data, where `n_samples` is the number of samples\n",
        "            and `n_features` is the number of features.\n",
        "        y : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
        "            Target values. If 2d array, this is the probability\n",
        "            distribution over the `n_classes` for each of the `n_samples`.\n",
        "        sample_weight : array-like of shape (n_samples,)\n",
        "            Array of weights that are assigned to individual samples.\n",
        "            If not provided, then each sample is given unit weight.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns the instance itself.\n",
        "\n",
        "        \"\"\"\n",
        "        attrs = [\n",
        "            \"gpu_\",\n",
        "            \"corpus_\",\n",
        "            \"classes_\",\n",
        "            \"n_features_in_\"\n",
        "        ]\n",
        "\n",
        "        for attr in attrs:\n",
        "            if hasattr(self, attr):\n",
        "                delattr(self, attr)\n",
        "\n",
        "        return self.partial_fit(X, y, classes=y, sample_weight=sample_weight)\n",
        "\n",
        "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
        "        \"\"\"Incremental fit on a batch of samples.\n",
        "\n",
        "        This method is expected to be called several times consecutively on different chunks of a dataset so\n",
        "        as to implement out-of-core or online learning.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Training data, where `n_samples` is the number of samples\n",
        "            and `n_features` is the number of features.\n",
        "        y : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
        "            Target values. If 2d array, this is the probability\n",
        "            distribution over the `n_classes` for each of the `n_samples`.\n",
        "        classes : array-like of shape (n_classes,)\n",
        "            List of all the classes that can possibly appear in the `y` vector.\n",
        "            Must be provided at the first call to `partial_fit`, can be omitted in subsequent calls.\n",
        "        sample_weight : array-like of shape (n_samples,)\n",
        "            Array of weights that are assigned to individual samples.\n",
        "            If not provided, then each sample is given unit weight.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns the instance itself.\n",
        "\n",
        "        \"\"\"\n",
        "        X, y = self._sanitize(X, y)\n",
        "\n",
        "        first_call = self._check_partial_fit_first_call(classes)\n",
        "        if first_call:\n",
        "            self.corpus_ = 0\n",
        "            self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        if not self._check_encoded(y):\n",
        "            y = self._one_hot_encoding(y)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = self._check_sample_weight(sample_weight, X)\n",
        "            y = self._multiply(y, sample_weight.reshape(-1, 1))\n",
        "\n",
        "        self.corpus_ += X.T @ self._multiply(y, self._power(self._sum(X, axis=1), -1))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Perform classification on the test data X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Test data, where `n_samples` is the number of samples\n",
        "            and `n_features` is the number of features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : ndarray of shape (n_samples,)\n",
        "            Predicted target classes for `X`.\n",
        "\n",
        "        \"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        idx = self._dense().argmax(proba, axis=1)\n",
        "\n",
        "        return self.classes_[idx]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Return probability estimates for the test data X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Test data, where `n_samples` is the number of samples\n",
        "            and `n_features` is the number of features.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y : ndarray of shape (n_samples, n_classes)\n",
        "            Returns the probability of the samples for each class in the model.\n",
        "            The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`.\n",
        "\n",
        "        \"\"\"\n",
        "        self._check_fitted()\n",
        "\n",
        "        X = self._sanitize(X)\n",
        "        u = self._power(self._power(X, self.a) @ self._weights(), 1. / self.a)\n",
        "        y = self._normalize(u, axis=1)\n",
        "\n",
        "        if self._sparse().issparse(y):\n",
        "            y = y.todense()\n",
        "\n",
        "        return self._dense().asarray(y)\n",
        "\n",
        "    def explain(self, X=None, sample_weight=None):\n",
        "        r\"\"\"Global and local explanation\n",
        "\n",
        "        For each test vector $`x`$, the $`a`$-th power of the unnormalized probability for the $`k`$-th class is\n",
        "        given by the matrix product:\n",
        "\n",
        "        ```math\n",
        "        u_k^a = \\sum_j W_{jk}x_j^a\n",
        "        ```\n",
        "        where $`W`$ is a matrix of non-negative weights that generally depends on the model's\n",
        "        hyper-parameters ($`a`$, $`b`$, $`h`$). The classification probabilities are obtained by\n",
        "        normalizing $`u`$ such that it sums up to $`1`$.\n",
        "\n",
        "        This method returns global or local feature importance weights, depending on `X`:\n",
        "\n",
        "        - When `X` is not provided, this method returns the global weights $`W`$.\n",
        "\n",
        "        - When `X` is a single sample,\n",
        "        this method returns a matrix of entries $`(j,k)`$ where each entry is given by $`W_{jk}x_j^a`$.\n",
        "\n",
        "        - When `X` contains multiple samples,\n",
        "        then the values above are computed for each sample and this method returns their weighted sum.\n",
        "        By default, each sample is given unit weight.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Test data, where `n_samples` is the number of samples\n",
        "            and `n_features` is the number of features. If not provided,\n",
        "            then global weights are returned.\n",
        "        sample_weight : array-like of shape (n_samples,)\n",
        "            Array of weights that are assigned to individual samples.\n",
        "            If not provided, then each sample is given unit weight.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        E : array-like of shape (n_features, n_classes)\n",
        "            Returns the feature importance for each class in the model.\n",
        "            The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`.\n",
        "\n",
        "        \"\"\"\n",
        "        self._check_fitted()\n",
        "\n",
        "        if X is None:\n",
        "            return self._weights()\n",
        "\n",
        "        X = self._sanitize(X)\n",
        "        X = self._normalize(X, axis=1)\n",
        "        X = self._power(X, self.a)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = self._check_sample_weight(sample_weight, X)\n",
        "            X = self._multiply(X, sample_weight.reshape(-1, 1))\n",
        "\n",
        "        return self._multiply(self._weights(), self._sum(X, axis=0).T)\n",
        "\n",
        "    def _dense(self):\n",
        "        return  numpy\n",
        "\n",
        "    def _sparse(self):\n",
        "        return scipy.sparse\n",
        "\n",
        "    def _weights(self):\n",
        "        P_jk = self.corpus_\n",
        "        if self.b != 0:\n",
        "            P_jk = self._multiply(P_jk, self._power(self._sum(self.corpus_, axis=0), -self.b))\n",
        "        if self.b != 1:\n",
        "            P_jk = self._multiply(P_jk, self._power(self._sum(self.corpus_, axis=1), self.b-1))\n",
        "\n",
        "        W_jk = self._power(P_jk, self.a)\n",
        "        if self.h != 0 and len(self.classes_) > 1:\n",
        "            P_jk = self._normalize(P_jk, axis=1)\n",
        "            H_j = 1 + self._sum(self._multiply(P_jk, self._log(P_jk)), axis=1) / self._dense().log(P_jk.shape[1])\n",
        "            W_jk = self._multiply(W_jk, self._power(H_j, self.h))\n",
        "\n",
        "        return W_jk\n",
        "\n",
        "    def _sum(self, x, axis):\n",
        "        if self._sparse().issparse(x):\n",
        "            return x.sum(axis=axis)\n",
        "\n",
        "        return self._dense().asarray(x).sum(axis=axis, keepdims=True)\n",
        "\n",
        "    def _multiply(self, x, y):\n",
        "        if self._sparse().issparse(x):\n",
        "            return x.multiply(y).tocsr()\n",
        "\n",
        "        if self._sparse().issparse(y):\n",
        "            return y.multiply(x).tocsr()\n",
        "\n",
        "        return self._dense().multiply(x, y)\n",
        "\n",
        "    def _power(self, x, p):\n",
        "        x = x.copy()\n",
        "\n",
        "        if self._sparse().issparse(x):\n",
        "            x.data = self._dense().power(x.data, p)\n",
        "\n",
        "        else:\n",
        "            nz = self._dense().nonzero(x)\n",
        "            x[nz] = self._dense().power(x[nz], p)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _log(self, x):\n",
        "        x = x.copy()\n",
        "\n",
        "        if self._sparse().issparse(x):\n",
        "            x.data = self._dense().log(x.data)\n",
        "\n",
        "        else:\n",
        "            nz = self._dense().nonzero(x)\n",
        "            x[nz] = self._dense().log(x[nz])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _normalize(self, x, axis, p=1.):\n",
        "        s = self._sum(x, axis)\n",
        "        n = self._power(s, -p)\n",
        "\n",
        "        return self._multiply(x, n)\n",
        "\n",
        "    def _sanitize(self, X, y=\"no_validation\"):\n",
        "        only_X = isinstance(y, str) and y == \"no_validation\"\n",
        "\n",
        "        gpu = self._check_gpu(X=X, y=y if not only_X else None)\n",
        "        if getattr(self, \"gpu_\", None) is None:\n",
        "            self.gpu_ = gpu\n",
        "\n",
        "        elif self.gpu_ != gpu:\n",
        "            raise ValueError(\n",
        "                \"X is not on the same device (CPU/GPU) as on last call \"\n",
        "                \"to partial_fit, was: %r\" % (self.gpu_, ))\n",
        "\n",
        "        if not self.gpu_:\n",
        "            kwargs = {\n",
        "                \"accept_sparse\": \"csr\",\n",
        "                \"reset\": False,\n",
        "                \"dtype\": (numpy.float32, numpy.float64)\n",
        "            }\n",
        "\n",
        "            if only_X:\n",
        "                X = super()._validate_data(X=X, **kwargs)\n",
        "\n",
        "            else:\n",
        "                X, y = super()._validate_data(X=X, y=y, multi_output=self._check_encoded(y), **kwargs)\n",
        "\n",
        "            if not self._check_non_negative(X):\n",
        "                raise ValueError(\"X must contain non-negative values\")\n",
        "\n",
        "        return X if only_X else (X, y)\n",
        "\n",
        "    def _unique_labels(self, y):\n",
        "        if self._check_encoded(y):\n",
        "            return self._dense().arange(0, y.shape[1])\n",
        "\n",
        "        elif self.gpu_:\n",
        "            return self._dense().unique(y)\n",
        "\n",
        "        else:\n",
        "            return unique_labels(y)\n",
        "\n",
        "    def _one_hot_encoding(self, y):\n",
        "        classes = self.classes_\n",
        "        n, m = len(y), len(classes)\n",
        "\n",
        "        if self.gpu_:\n",
        "            y = y.get()\n",
        "            classes = classes.get()\n",
        "\n",
        "        unseen = set(y) - set(classes)\n",
        "        if unseen:\n",
        "            raise ValueError(\n",
        "                \"`classes=%r` were not allowed on first call \"\n",
        "                \"to partial_fit\" % (unseen, ))\n",
        "\n",
        "        idx = {c: i for i, c in enumerate(classes)}\n",
        "        col = self._dense().array([idx[c] for c in y])\n",
        "        row = self._dense().array(range(0, n))\n",
        "        val = self._dense().ones(n)\n",
        "\n",
        "        return self._sparse().csr_matrix((val, (row, col)), shape=(n, m))\n",
        "\n",
        "    def _check_encoded(self, y):\n",
        "        return self._sparse().issparse(y) or (getattr(y, \"ndim\", 0) == 2 and y.shape[1] > 1)\n",
        "\n",
        "    def _check_non_negative(self, X):\n",
        "        if self._sparse().issparse(X):\n",
        "            if self._dense().any(X.data < 0):\n",
        "                return False\n",
        "\n",
        "        elif self._dense().any(X < 0):\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _check_sample_weight(self, sample_weight, X):\n",
        "        if self.gpu_:\n",
        "            return sample_weight\n",
        "\n",
        "        return _check_sample_weight(sample_weight=sample_weight, X=X)\n",
        "\n",
        "    def _check_partial_fit_first_call(self, classes):\n",
        "        if getattr(self, \"classes_\", None) is None and classes is None:\n",
        "            raise ValueError(\"classes must be passed on the first call to partial_fit\")\n",
        "\n",
        "        elif classes is not None:\n",
        "            classes = self._unique_labels(classes)\n",
        "\n",
        "            if getattr(self, \"classes_\", None) is not None:\n",
        "                if not self._dense().array_equal(self.classes_, classes):\n",
        "                    raise ValueError(\n",
        "                        \"`classes=%r` is not the same as on last call \"\n",
        "                        \"to partial_fit, was: %r\" % (classes, self.classes_))\n",
        "\n",
        "            else:\n",
        "                self.classes_ = classes\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _check_gpu(self, X, y=None):\n",
        "        return False\n",
        "\n",
        "    def _check_fitted(self):\n",
        "        if getattr(self, \"corpus_\", None) is None:\n",
        "            raise NotFittedError(\n",
        "                f\"This {self.__class__.__name__} instance is not fitted yet. \"\n",
        "                \"Call 'fit' with appropriate arguments before using this estimator\")\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {\n",
        "            'requires_y': True,\n",
        "            'requires_positive_X': True,\n",
        "            'X_types': ['2darray', 'sparse'],\n",
        "            '_xfail_checks': {\n",
        "                'check_classifiers_classes':\n",
        "                    'This is a pathological data set for BornClassifier. '\n",
        "                    'For some specific cases, it predicts less classes than expected',\n",
        "                'check_classifiers_train':\n",
        "                    'Test fails because of negative values in X'\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions"
      ],
      "metadata": {
        "id": "__Y8fznPNtUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(number, column):\n",
        "    encoding = [0] * len(np.unique(column))\n",
        "    encoding[number] = 1\n",
        "    return encoding\n",
        "\n",
        "def show_confusion_matrix(set_):\n",
        "  print(\"true positive\", set_[3])\n",
        "  print(\"true negative\", set_[0])\n",
        "  print(\"false positive\", set_[1])\n",
        "  print(\"false negative\", set_[2])\n",
        "\n",
        "# def scaling_features(array_):\n",
        "#   return np.abs(array_)\n",
        "\n",
        "# rescale betwenn [0; max+min]\n",
        "def scaling_features(array_):\n",
        "    min_value = array_.min()\n",
        "    if min_value < 0:\n",
        "        return array_ - min_value\n",
        "    else:\n",
        "        return array_\n",
        "\n",
        "def convert_to_gray_scale(image):\n",
        "  try:\n",
        "    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    return grayscale_image\n",
        "  except:\n",
        "    return image\n",
        "\n",
        "def prepare_images(img):\n",
        "    gray_image = convert_to_gray_scale(img).flatten()\n",
        "    return np.array(gray_image)"
      ],
      "metadata": {
        "id": "HH51d1WpTamr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data"
      ],
      "metadata": {
        "id": "sklr0mrzuLeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Tesi/MEDICAL IMAGING/'\n",
        "file_ = 'Skin_Cancer_binary'\n",
        "path_file = path + file_ +'.h5'\n",
        "df = pd.read_hdf(path_file, 'df')\n",
        "\n",
        "df['label'] = [one_hot_encode(lab, df.label) for lab in df['label']]\n",
        "#train e test\n",
        "df['preprocess_img'] = [prepare_images(img) for img in df.image]\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])"
      ],
      "metadata": {
        "id": "Cu8WxBzMAhYm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.image[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q74CiFi1AxaB",
        "outputId": "9cf1585d-9d4e-4956-87a7-c329668b6050"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dict learning"
      ],
      "metadata": {
        "id": "vSChfldhBVcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#learn the dict\n",
        "n_components = 100\n",
        "dict_learner = DictionaryLearning(n_components=n_components, max_iter=5, random_state=0, verbose = 1)\n",
        "A_train = dict_learner.fit_transform(np.array(list(df_train.preprocess_img)))\n",
        "#use the dict\n",
        "A_test = dict_learner.transform(np.array(list(df_test.preprocess_img)))\n",
        "#label as arrays\n",
        "y_train = np.array(list(df_train.label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKtnrW6cBdZ3",
        "outputId": "fde83537-5f11-4a18-a0c3-4a0a68ab1b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[dict_learning] ....."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "classifier = BornClassifier()\n",
        "classifier.fit(scaling_features(A_train), y_train)\n",
        "#test\n",
        "y_pred = classifier.predict(scaling_features(A_test))\n",
        "y_obs = [np.argmax(lab) for lab in df_test.label]"
      ],
      "metadata": {
        "id": "hZxuhm-ludGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy:\n",
        "accuracy_score(y_obs, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D72JK9Ozy7o9",
        "outputId": "3986f66d-51b4-42bb-864b-5525f6055851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8042935596605092"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_obs, y_pred.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7drg1jQG_CvB",
        "outputId": "ee2f6cda-8943-48cd-cdc8-965fbf72f7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1611,    1],\n",
              "       [ 391,    0]])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8NfnRCMpH-yq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}